{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8229d3",
   "metadata": {},
   "source": [
    "# Summary for Transformer-Based Emotion Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5219209",
   "metadata": {},
   "source": [
    "Here is the synthesized summary of the entire pipeline in professional, academic-style bullet points:\n",
    "\n",
    "    Environment Setup\n",
    "       - Configures essential Python libraries, Hugging Face Transformers, and NLP resources (e.g., NLTK).\n",
    "       - Establishes computational environment with GPU/CPU allocation for optimized training performance.\n",
    "       \n",
    "    Data Preparation\n",
    "       - Loads and cleans the raw dataset, removing duplicates and handling missing values.\n",
    "       - Balances class distribution using downsampling and optional GPT-2-based text augmentation.\n",
    "       - Applies preprocessing (e.g., lemmatization, stopword removal) and stratified splitting into training, validation, and test sets.\n",
    "       \n",
    "    Model Fine-Tuning\n",
    "       - Initializes BERT, XLNet, and GPT-2 models with appropriate tokenizers and padding strategies.\n",
    "       - Uses Optuna for automated hyperparameter tuning (learning rate, weight decay, warmup ratio).\n",
    "       - Trains each model on the training set while monitoring validation performance with early stopping.\n",
    "\n",
    "    Model Evaluation\n",
    "       -  Evaluate trained models on a held-out test set using accuracy and macro-averaged F1 score.\n",
    "       -  Generates detailed classification reports and visualizes confusion matrices for interpretability.\n",
    "\n",
    "    Reporting and Visualization\n",
    "       - It saves evaluation results and training time and is the best hyperparameter for structured CSV and JSON files.\n",
    "       - Produces comparative bar plots of F1 scores across models for clear visual assessment.\n",
    "       - Outputs comprehensive logs and formatted summaries suitable for thesis documentation and presentations.\n",
    "       - This structured flow ensures a reproducible, transparent, and empirically sound methodology for benchmarking transformer models in multi-class emotion classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745dce7",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dcd3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7603516a",
   "metadata": {},
   "source": [
    "# Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a76681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# pip install --upgrade transformers datasets accelerate optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53ff2e",
   "metadata": {},
   "source": [
    "## Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "# Authenticate with Hugging Face Hub\n",
    "login(token=\"hf_Nj******\")\n",
    "\n",
    "os.environ['HF_TOKEN']=\"hf_Nj******\"\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"hf_Nj******\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3844aa",
   "metadata": {},
   "source": [
    "## Library Imports "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6f5a7",
   "metadata": {},
   "source": [
    "### Data manipulation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                    # For handling dataframes and reading/writing CSVs\n",
    "import re                              # Text cleaning with regular expressions\n",
    "import nltk                            # Text preparation with the Natural Language Toolkit\n",
    "from nltk.corpus import stopwords       # Common stopwords for filtering\n",
    "from nltk.stem import WordNetLemmatizer  # Lemmatization to normalize words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff4483",
   "metadata": {},
   "source": [
    "### Data splitting and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee641b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split                        # Stratified train/val/test splitting\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score  # Evaluation metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight              # Compute class weights to address imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea387ec",
   "metadata": {},
   "source": [
    "### Hugging Face Transformers for tokenization, modeling, and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,         # BERT model and tokenizer for sequence classification\n",
    "    XLNetTokenizer, XLNetForSequenceClassification,       # XLNet model and tokenizer\n",
    "    GPT2Tokenizer, GPT2ForSequenceClassification,         # GPT-2 model adapted for classification\n",
    "    Trainer, DataCollatorWithPadding,                     # Training interface and dynamic padding\n",
    "    AutoTokenizer, AutoModelForCausalLM,                  # For generative text augmentation using GPT-2\n",
    "    pipeline, get_scheduler,                              # Utilities for text generation and learning rate scheduling\n",
    "    TrainingArguments,                                    # Training configuration container\n",
    "    EarlyStoppingCallback                                 # Callback for early stopping during training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d766e2d",
   "metadata": {},
   "source": [
    "### Deep learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # PyTorch for tensor operations and GPU acceleration\n",
    "import numpy as np  # Numericals operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5bc03b",
   "metadata": {},
   "source": [
    " ### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66054a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # For plotting performance metrics and confusion matrices\n",
    "import seaborn as sns            # Enhanced data visualization (used for confusion matrix heatmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4023f66",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import optuna   # Framework for automated hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308cf0d7",
   "metadata": {},
   "source": [
    "### Utility modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72abab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time  # Runtime profiling\n",
    "import json  # Exporting results and configurations to JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920ee0e",
   "metadata": {},
   "source": [
    "### Hardware Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure CUDA device (for multi-GPU environments, \"0\" specifies the first GPU)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Dynamically select device: use GPU if available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# In case a GPU is not available, configure PyTorch to use a high number of CPU threads for performance\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Using CPU with 96 threads\")  # This assumes a high-core-count CPU (e.g., in HPC environments)\n",
    "    torch.set_num_threads(96)\n",
    "else:\n",
    "    # Print the name of the available GPU\n",
    "    print(\"We have a GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23477d2",
   "metadata": {},
   "source": [
    "### Natural Language Toolkit (NLTK) Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16db053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords list for text cleaning\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download WordNet corpus for lemmatization\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d48b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97e0a526",
   "metadata": {},
   "source": [
    "# Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18d17c8",
   "metadata": {},
   "source": [
    "Objective :-\n",
    "    \n",
    "    Useful when exporting evaluation results (e.g., confusion matrix) to JSON format, which does not support NumPy arrays directly.\n",
    "    Essential for preparing text inputs before tokenization and modeling. Improves model generalization by reducing noise and redundancy in the input text.\n",
    "    Helpful in classification tasks to map numerical label encodings to their string representations and count the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df27dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a NumPy array to a Python list for JSON serialization.\n",
    "def convert_ndarray(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts label information from the dataset\n",
    "def get_label_info(df, label_column='label'):\n",
    "    label_names = df[label_column].astype('category').cat.categories.tolist()\n",
    "    num_labels = len(label_names)\n",
    "    return label_names, num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c8908",
   "metadata": {},
   "source": [
    "## Preprocesses text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7e6e5",
   "metadata": {},
   "source": [
    "Objective :-\n",
    "            \n",
    "          Cleans and preprocesses raw text for NLP tasks:\n",
    "               Eliminates special characters, hashtags, URLs, and mentions; lowercases text\n",
    "               Lemmatisation is used to tokenise and eliminate stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791858bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses text data.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "    text = re.sub(r'#[A-Za-z0-9_]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0476c4",
   "metadata": {},
   "source": [
    "## Augment_minority_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b49e55",
   "metadata": {},
   "source": [
    "Objective :-\n",
    "\n",
    "  This function is essential for addressing class imbalance in supervised emotion classification by generating synthetic samples for underrepresented classes using GPT-style models. This enhances model generalization and improves performance on minority labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19bea47",
   "metadata": {},
   "source": [
    "Augments minority classes using a causal language model (e.g., DistilGPT-2).\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Original dataset.\n",
    "    - text_column (str): Column containing input text data.\n",
    "    - label_column (str): Column containing emotion labels.\n",
    "    - num_augmentations (int): Number of augmentation prompts per sample.\n",
    "    - output_csv_path (str): File path to save/load the augmented dataset.\n",
    "    - model_name (str): Name of the Hugging Face model used for generation.\n",
    "    - device (str): 'cuda' or 'cpu'.\n",
    "    - max_new_tokens (int): Maximum tokens to generate per sample.\n",
    "    - batch_size (int): Number of prompts processed per batch.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Original data combined with generated synthetic samples.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93efb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_minority_classes(df, text_column='text', label_column='label', num_augmentations=1,output_csv_path='augmented_dataset.csv', model_name='distilgpt2', device='cuda', max_new_tokens=80, batch_size=32):\n",
    "    if os.path.exists(output_csv_path):\n",
    "        print(f\"Loading existing augmented dataset from: {output_csv_path}\")\n",
    "        return pd.read_csv(output_csv_path)\n",
    "\n",
    "    print(\"Generating new augmented dataset (batched)...\")\n",
    "    label_counts = df[label_column].value_counts()\n",
    "    max_count = label_counts.max()\n",
    "    augmented_rows = []\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for label, count in label_counts.items():\n",
    "        if count >= max_count:\n",
    "            continue\n",
    "\n",
    "        minority_df = df[df[label_column] == label]\n",
    "        required = max_count - count\n",
    "        per_sample = max(1, required // len(minority_df))\n",
    "\n",
    "        prompts = []\n",
    "        for _, row in minority_df.iterrows():\n",
    "            for _ in range(min(num_augmentations, per_sample)):\n",
    "                prompt = f\"{row[text_column]} Emotion: {label}.\"\n",
    "                prompts.append(prompt)\n",
    "\n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batch_prompts = prompts[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_prompts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    #max_length=max_length,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    top_p=0.95,\n",
    "                    num_return_sequences=1,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "            for prompt, gen_text in zip(batch_prompts, decoded):\n",
    "                continuation = gen_text.replace(prompt, \"\").strip()\n",
    "                augmented_rows.append({text_column: continuation, label_column: label})\n",
    "\n",
    "    augmented_df = pd.concat([df, pd.DataFrame(augmented_rows)], ignore_index=True)\n",
    "    augmented_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Saved augmented dataset to {output_csv_path}\")\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7135dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a74f5da",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b9aaf",
   "metadata": {},
   "source": [
    "Objective :-\n",
    "\n",
    "    The text is cleaned and preprocessed before tokenization.\n",
    "    The dataset is stratified, maintaining proportional label distributions across training, validation, and test sets.\n",
    "    Augmentation is integrated as an optional preprocessing step to address class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715a35e",
   "metadata": {},
   "source": [
    "divides the dataset in to training, validation, and test sets and preprocesses the text.\n",
    "\n",
    "    Parameters:\n",
    "    - df : The initial data set containing text and labels.\n",
    "    - text_column : Column name containing textual input data.\n",
    "    - label_column : Column name containing emotion labels.\n",
    "    - train_size : Proportion of data used for training.\n",
    "    - val_size : Proportion of data  used for validtion.\n",
    "    - test_size : Proportion of data  used for test .\n",
    "    - augment : Whether to perform augmentation on minority classes.\n",
    "    - num_augmentations : Number of synthetic samples per instance if augmentation is applied.\n",
    "    - random_state : Random seed for reproducibility.\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e030e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_datasets(df, text_column='text', label_column='label', train_size=0.7, val_size=0.15, test_size=0.15, augment=False, num_augmentations=2, random_state=42):\n",
    "    \n",
    "    if augment:     \n",
    "         df = augment_minority_classes(df, text_column, label_column, num_augmentations)\n",
    "\n",
    "    X = df[text_column].apply(preprocess_text).tolist()\n",
    "    y = df[label_column].astype('category').cat.codes.tolist()\n",
    "    label_names = df[label_column].astype('category').cat.categories.tolist()\n",
    "    #num_labels = len(label_names)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_size, random_state=random_state, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_size / (1 - train_size), random_state=random_state, stratify=y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c516f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97285af6",
   "metadata": {},
   "source": [
    "# Tokenization and Dataset Wrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5650b1fe",
   "metadata": {},
   "source": [
    "Objective :-\n",
    "\n",
    "    Facilitates compatibility with the Hugging Face Trainer API by structuring the data in a format compliant with PyTorch’s Dataset class. This approach ensures that text inputs are efficiently tokenized and converted into tensors, while corresponding labels are integrated to support supervised learning workflows during model fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c75c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Custom dataset class for emotion classification tasks.\n",
    "# Wraps tokenized inputs and labels into a PyTorch-compatible Dataset structure for use with Hugging Face's Trainer API.\n",
    "\n",
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset for emotion data.\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff11fc",
   "metadata": {},
   "source": [
    "## Tokenizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd572a",
   "metadata": {},
   "source": [
    "Tokenizes and encodes raw text data for use with Transformer models.\n",
    "\n",
    "    Parameters:\n",
    "    - tokenizer: Hugging Face tokenizer corresponding to the model (e.g., BERT, XLNet, GPT-2).\n",
    "    - X_train, X_val, X_test: Preprocessed textual data splits.\n",
    "    - y_train, y_val, y_test: Integer-encoded label splits.\n",
    "    - max_length: Maximum sequence length after padding/truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9daa2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(tokenizer, X_train, X_val, X_test, y_train, y_val, y_test, max_length=128):\n",
    "    \"\"\"Tokenizes and encodes text data.\"\"\"\n",
    "    # Ensure tokenizer has pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token if hasattr(tokenizer, \"eos_token\") else '[PAD]'\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id if hasattr(tokenizer, \"eos_token_id\") else 0\n",
    "\n",
    "    tokenizer.padding_side = 'right'  # Ensures consistency\n",
    "\n",
    "    train_encodings = tokenizer(X_train, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "    val_encodings = tokenizer(X_val, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "    test_encodings = tokenizer(X_test, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "    # Add labels to encodings\n",
    "    train_encodings['labels'] = y_train\n",
    "    val_encodings['labels'] = y_val\n",
    "    test_encodings['labels'] = y_test\n",
    "    return train_encodings, val_encodings, test_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c2e48",
   "metadata": {},
   "source": [
    "# Trainer Configuration and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08317187",
   "metadata": {},
   "source": [
    "Objective :-\n",
    "\n",
    "        This function setup enables robust and fair model training, particularly in the presence of class imbalance. It incorporates:\n",
    "        Custom weighted loss to mitigate label skew.\n",
    "        Early stopping to prevent overfitting.\n",
    "        F1-macro optimization, suitable for multi-class emotion classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58c4ed",
   "metadata": {},
   "source": [
    "## Create_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e79f9",
   "metadata": {},
   "source": [
    "Configures and returns a Hugging Face `Trainer` instance tailored for emotion classification.\n",
    "\n",
    "    Features:\n",
    "    - Handles class imbalance using weighted loss.\n",
    "    - Applies learning rate scheduling and early stopping.\n",
    "    - Uses DataCollatorWithPadding for dynamic padding.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Pre-trained transformer model (BERT, XLNet, or GPT-2).\n",
    "    - tokenizer: Tokenizer corresponding to the selected model.\n",
    "    - train_encodings, val_encodings: Tokenized datasets.\n",
    "    - y_train: List of training labels (used for class weight calculation).\n",
    "    - weight_decay: Weight decay coefficient for regularization.\n",
    "    - warmup_ratio: Ratio of warm-up steps for scheduler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbfb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, tokenizer, train_encodings, val_encodings, y_train, batch_size=16, epochs=3, weight_decay=0.01, learning_rate=5e-5, warmup_ratio=0.06):\n",
    "    \"\"\"Creates and configures a Trainer with class weights and learning rate scheduler.\"\"\"\n",
    "    train_dataset = EmotionDataset(train_encodings, train_encodings.pop('labels'))\n",
    "    val_dataset = EmotionDataset(val_encodings, val_encodings.pop('labels'))\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length', max_length=128)\n",
    "\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    \n",
    "    def compute_weighted_loss(model, inputs, return_outputs=False,**kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs, return_dict=True)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_loss(model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',                        # check points saved on output file folder\n",
    "        num_train_epochs=epochs,                       # Num training epochs\n",
    "        per_device_train_batch_size=batch_size,        # Train batch size\n",
    "        per_device_eval_batch_size=batch_size,         # Eval batch size\n",
    "        warmup_ratio=warmup_ratio,                     # Warm-up stp LR scheduler\n",
    "        weight_decay=weight_decay,                     # Regularization to prevent overfitting\n",
    "        learning_rate=learning_rate,                   # Optimizer learning rate\n",
    "        logging_dir='./logs',                          # Directory to store logs\n",
    "        logging_steps=10,                              # Frequency of logging\n",
    "        eval_strategy='epoch',                         # Run evaluation every epoch\n",
    "        save_strategy='epoch',                         # Save model every epoch\n",
    "        save_steps=500,                                # Save model every 500 stps\n",
    "        eval_steps=500,                                # Run evaluation every 500 stps\n",
    "        load_best_model_at_end=True,                   # Automatically load the optimal checkpoint according to the metric\n",
    "        metric_for_best_model='eval_f1_macro',         # Metric to track best model\n",
    "        fp16=True,                                     # Use 16-bit precision \n",
    "        greater_is_better=True                         # Optimize towards higher score\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        #compute_loss=compute_weighted_loss,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "    trainer.compute_loss = compute_weighted_loss\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534c85c",
   "metadata": {},
   "source": [
    "## Compute_metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad47fa5",
   "metadata": {},
   "source": [
    " Computes key evaluation metrics for classification:\n",
    " \n",
    "    - Accuracy\n",
    "    - Macro-averaged F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    return {'accuracy': accuracy, 'f1_macro': f1_macro}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446837e",
   "metadata": {},
   "source": [
    "#  Evaluation and Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd99e5d",
   "metadata": {},
   "source": [
    "Objective :-\n",
    "\n",
    "    This function facilitates a comprehensive performance assessment by generating quantitative metrics and visualizing classification outcomes, thereby enabling both statistical evaluation and diagnostic error analysis. Additionally, it supports reproducibility and deployment by systematically exporting the trained model and tokenizer in a standardized format compatible with the Hugging Face ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f58f42",
   "metadata": {},
   "source": [
    "## evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb456be",
   "metadata": {},
   "source": [
    " Evaluates a fine-tuned transformer model on a held-out test set, computes classification metrics, and generates a confusion matrix plot.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained transformer-based classification model.\n",
    "    - tokenizer: Tokenizer used for encoding the dataset.\n",
    "    - test_encodings : Tokenized test data with input tensors.\n",
    "    - y_test : Ground truth labels for the test set.\n",
    "    - label_names : Original class names for report readability.\n",
    "    - model_name : Descriptive model identifier (e.g., 'BERT', 'GPT-2').\n",
    "    - save_folder : Destination folder for saving visual outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13145c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, test_encodings, y_test, label_names, model_name, save_folder):\n",
    "    \n",
    "    # Convert encoded test inputs into a PyTorch-compatible dataset\n",
    "    test_dataset = EmotionDataset(test_encodings, test_encodings.pop('labels'))\n",
    "    \n",
    "    # Reuse Hugging Face Trainer for efficient prediction\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    report = classification_report(y_test, y_pred, target_names=label_names, zero_division=1)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  Accuracy: {accuracy}\")\n",
    "    print(f\"  F1 Macro: {f1_macro}\")\n",
    "    print(f\"  Classification Report:\\n{report}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.savefig(os.path.join(save_folder, f'{model_name}_confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "    return {'accuracy': accuracy, 'f1_macro': f1_macro, 'report': report, 'cm': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c620e5",
   "metadata": {},
   "source": [
    "## save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the fine-tuned model and its tokenizer to disk for reuse or deployment.\n",
    "def save_model(model, tokenizer, model_save_path):\n",
    "    \"\"\"Saves the trained model and tokenizer.\"\"\"\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    print(f\"\\nModel and tokenizer saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a0e6a",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e7971",
   "metadata": {},
   "source": [
    " Objective :-    \n",
    " \n",
    "     This function enables automated hyperparameter search via Optuna, targeting optimal training performance by maximizing the macro-averaged F1 score. It dynamically evaluates multiple model configurations across learning rate, weight decay, and warmup ratio, thereby enhancing model generalizability and robustness—particularly critical in imbalanced multi-class emotion classification tasks.\n",
    " \n",
    " Defines the objective function for Optuna to optimize hyperparameters\n",
    "    in transformer-based emotion classification models.\n",
    "\n",
    "    Parameters:\n",
    "    - trial: Optuna trial object for suggesting hyperparameters.\n",
    "    - model_name : Identifier of the model type ('BERT', 'XLNet', 'GPT-2').\n",
    "    - tokenizer: Tokenizer instance corresponding to the selected model.\n",
    "    - X_train_encodings : Tokenized training input data.\n",
    "    - X_val_encodings : Tokenized validation input data.\n",
    "    - y_train : Encoded training labels.\n",
    "    - batch_size : Batch size for training.\n",
    "    - epochs : Number of epochs for model training.\n",
    "    - num_labels : Number of output classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19791cc4",
   "metadata": {},
   "source": [
    "## objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab38773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model_name, tokenizer, X_train_encodings, X_val_encodings, y_train, batch_size, epochs,num_labels):\n",
    "    \"\"\"Objective function for Optuna hyperparameter optimization.\"\"\"\n",
    "\n",
    "    train_encodings_copy = X_train_encodings.copy()  # Create a copy\n",
    "    val_encodings_copy = X_val_encodings.copy()  # Create a copy\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.0, 0.2)\n",
    "\n",
    "    if model_name == 'BERT':\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "    elif model_name == 'XLNet':\n",
    "        model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=num_labels)\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    elif model_name == 'GPT-2':\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.padding_side = 'right'\n",
    "        model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=num_labels)\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model name: {model_name}\")\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    trainer = create_trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_encodings=train_encodings_copy,\n",
    "        val_encodings=val_encodings_copy,\n",
    "        y_train=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=warmup_ratio\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    return eval_results['eval_f1_macro']  # Optimize for F1 macro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2955bc93",
   "metadata": {},
   "source": [
    "# Unified Fine-tuning and Evaluation Routine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6818287b",
   "metadata": {},
   "source": [
    "## fine_tune_and_evaluate\n",
    "\n",
    "Objective :-\n",
    "\n",
    "\n",
    "        This function encapsulates the end-to-end training workflow:\n",
    "        Ensures class balance through augmentation.\n",
    "        Uses Optuna to discover optimal hyperparameters.\n",
    "        Fine-tunes and evaluates pre-trained LLMs for multi-class emotion detection.\n",
    "        Produces reproducible and deployable model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b1e8c",
   "metadata": {},
   "source": [
    "Orchestrates the full workflow for fine-tuning and evaluating a transformer-based emotion classification model.\n",
    "\n",
    "    Steps included:\n",
    "    - Data preparation (including optional augmentation)\n",
    "    - Tokenization\n",
    "    - Optuna-based hyperparameter tuning\n",
    "    - Model training with best parameters\n",
    "    - Evaluation on a held-out test set\n",
    "    - Saving of trained model and tokenizer\n",
    "\n",
    "    Parameters:\n",
    "    - df : Dataset with 'text' and 'label' columns.\n",
    "    - model_name : One of {'BERT', 'XLNet', 'GPT-2'}.\n",
    "    - tokenizer: Corresponding tokenizer for the model.\n",
    "    - model_save_path : Path to save the trained model and tokenizer.\n",
    "    - augment : Whether to apply data augmentation to balance classes.\n",
    "    - num_augmentations : Number of augmentations per minority instance.\n",
    "    - epochs : Number of training epochs.\n",
    "    - batch_size : Batch size during training and evaluation.\n",
    "    - n_trials : Number of Optuna trials for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2340444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_and_evaluate(df, model_name, tokenizer, model_save_path, augment=False, num_augmentations=2, epochs=3, batch_size=16, n_trials=10):\n",
    "    \"\"\"Fine-tunes and evaluates a given model.\"\"\"\n",
    "    if model_name == 'GPT-2':\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, label_names = create_datasets(df, augment=augment, num_augmentations=num_augmentations)\n",
    "\n",
    "    X_train_encodings, X_val_encodings, X_test_encodings = tokenize_data(tokenizer, X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "    label_names = df['label'].astype('category').cat.categories.tolist()\n",
    "    num_labels = len(label_names)\n",
    "\n",
    "    label_names, num_labels = get_label_info(df, label_column='label')\n",
    "    print(\"Label names:\", label_names)\n",
    "    print(\"Number of classes (num_labels):\", num_labels)\n",
    "\n",
    "    # Optuna optimization\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, model_name, tokenizer, X_train_encodings, X_val_encodings, y_train, batch_size, epochs,num_labels), n_trials=n_trials)\n",
    "\n",
    "    print(f\"\\nBest hyperparameters for {model_name}:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    # Train with best hyperparameters\n",
    "    best_model = None\n",
    "    if model_name == 'BERT':\n",
    "        best_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)#len(np.unique(y_train))\n",
    "    elif model_name == 'XLNet':\n",
    "        best_model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=num_labels)\n",
    "        best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    elif model_name == 'GPT-2':\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.padding_side = 'right'\n",
    "\n",
    "        best_model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=num_labels)\n",
    "        best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model name: {model_name}\")\n",
    "    \n",
    "    best_model.to(device)\n",
    "\n",
    "    best_trainer = create_trainer(\n",
    "        model=best_model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_encodings=X_train_encodings,\n",
    "        val_encodings=X_val_encodings,\n",
    "        y_train=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        learning_rate=study.best_params['learning_rate'],\n",
    "        weight_decay=study.best_params['weight_decay'],\n",
    "        warmup_ratio=study.best_params['warmup_ratio']\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time for {model_name}: {training_time:.2f} seconds\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    eval_results = evaluate_model(best_model, tokenizer, X_test_encodings, y_test, label_names, model_name, model_save_path)\n",
    "    eval_results['optuna_params'] = study.best_params\n",
    "    eval_results['training_time'] = training_time\n",
    "\n",
    "    save_model(best_model, tokenizer, model_save_path)\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c65066",
   "metadata": {},
   "source": [
    "# Pipeline Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d200223",
   "metadata": {},
   "source": [
    "## load_and_evaluate_all_models\n",
    "\n",
    "Objective :-\n",
    "\n",
    "            This function automates the end-to-end benchmarking of transformer models (BERT, XLNet, GPT-2) for emotion classification by:\n",
    "            Ensuring a balanced dataset using proportional downsampling.\n",
    "            Running consistent training and evaluation routines with hyperparameter optimization.\n",
    "            Producing reproducible results, model artifacts, and visualizations for reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71918d",
   "metadata": {},
   "source": [
    "Loads data, balances classes, fine-tunes three transformer models (BERT, XLNet, GPT-2),\n",
    "    evaluates their performance, and stores the results for comparison.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path : Path to the input CSV file (not directly used due to hardcoded file).\n",
    "    - augment : Whether to apply data augmentation for class balancing.\n",
    "    - num_augmentations : Number of augmentations per minority sample.\n",
    "    - epochs : Number of train epochs for each model.\n",
    "    - batch__size : Batchwise size used in train and eval.\n",
    "    - n_trials : Num of Optuna trials for hyperparameter search.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f501ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_all_models(file_path='emotion.csv', augment=False, num_augmentations=2, epochs=3, batch_size=16, n_trials=10):\n",
    "    \"\"\"Loads, preprocesses, fine-tunes, evaluates, and saves BERT, XLNet, and GPT-2 models.\"\"\"\n",
    "\n",
    "    try:\n",
    "#        from google.colab import drive\n",
    "       # drive.mount('/content/drive')\n",
    "        df_org = pd.read_csv('emotion_sentimen_dataset.csv')\n",
    "        df_org = df_org[['text', 'Emotion']]\n",
    "        df_org=df_org.drop_duplicates()\n",
    "        df = df_org[[\"text\", \"Emotion\"]]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Original label distribution:\\n\", df_org['Emotion'].value_counts(normalize=True))\n",
    "\n",
    "    target_rows = 300000\n",
    "    emotion_distribution = df_org['Emotion'].value_counts(normalize=True)*100\n",
    "    \n",
    "    #emotion_distribution\n",
    "    # Compute the number of rows per emotion\n",
    "    new_emotion_counts = (emotion_distribution * target_rows / 100).astype(int)\n",
    "    #new_emotion_counts = new_emotion_counts.apply(lambda x: max(x, 2)) # This line ensures each emotion has at least 2 instances\n",
    "\n",
    "    difference = target_rows - new_emotion_counts.sum()\n",
    "    new_emotion_counts.iloc[0] += difference  # Adjust the most frequent emotion\n",
    "\n",
    "    # Create a new DataFrame with the same distribution as the original dataset\n",
    "    new_df_balanced = pd.DataFrame()\n",
    "\n",
    "    # Generate synthetic data maintaining the original emotion distribution\n",
    "    new_df_balanced = pd.DataFrame()\n",
    "    for emotion, count in new_emotion_counts.items():\n",
    "        subset = df_org[df_org[\"Emotion\"] == emotion].sample(n=count, replace=False, random_state=42)\n",
    "        new_df_balanced = pd.concat([new_df_balanced, subset])\n",
    "\n",
    "    # Shuffle the new dataset to mix emotions\n",
    "    new_df_balanced = new_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    new_df_balanced['Emotion'].value_counts(normalize=True)\n",
    "    print(\"Balanced label distribution:\\n\", new_df_balanced['Emotion'].value_counts(normalize=True))\n",
    "    \n",
    "    df = new_df_balanced[['text', 'Emotion']].rename(columns={'Emotion': 'label'}).dropna()\n",
    "\n",
    "    # Original distribution (normalized)\n",
    "    original_dist = df_org['Emotion'].value_counts(normalize=True).sort_index()\n",
    "    # Downsampled distribution (normalized)\n",
    "    downsampled_dist = new_df_balanced['Emotion'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "    # Combine into a single DataFrame\n",
    "    comparison_df = pd.DataFrame({'Original (%)': (original_dist * 100).round(2),'Downsampled (%)': (downsampled_dist * 100).round(2)})\n",
    "    print(\"\\n Class Distribution Comparison (Original vs Downsampled):\",comparison_df)\n",
    "    \n",
    "\n",
    "    # Create folders\n",
    "    bert_folder = './bert_emotion_model'\n",
    "    xlnet_folder = './xlnet_emotion_model'\n",
    "    gpt2_folder = './gpt2_emotion_model'\n",
    "\n",
    "    os.makedirs(bert_folder, exist_ok=True)\n",
    "    os.makedirs(xlnet_folder, exist_ok=True)\n",
    "    os.makedirs(gpt2_folder, exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # BERT\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    results['BERT'] = fine_tune_and_evaluate(df.copy(), 'BERT', bert_tokenizer, bert_folder, augment, num_augmentations, epochs, batch_size, n_trials)\n",
    "\n",
    "    # XLNet\n",
    "    xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "    # Proper padding setup\n",
    "    xlnet_tokenizer.pad_token = xlnet_tokenizer.eos_token if xlnet_tokenizer.pad_token is None else xlnet_tokenizer.pad_token\n",
    "    xlnet_tokenizer.pad_token_id = xlnet_tokenizer.convert_tokens_to_ids(xlnet_tokenizer.pad_token)\n",
    "    xlnet_tokenizer.padding_side = 'right'\n",
    "    results['XLNet'] = fine_tune_and_evaluate(df.copy(), 'XLNet', xlnet_tokenizer, xlnet_folder, augment, num_augmentations, epochs, batch_size, n_trials)\n",
    "\n",
    "    # GPT-2\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    results['GPT-2'] = fine_tune_and_evaluate(df.copy(), 'GPT-2', gpt2_tokenizer, gpt2_folder, augment, num_augmentations, epochs, batch_size, n_trials)\n",
    "\n",
    "    # Find best model\n",
    "    best_model_name = max(results, key=lambda k: results[k]['f1_macro'])\n",
    "    print(f\"\\nBest model: {best_model_name} with F1 Macro: {results[best_model_name]['f1_macro']}\")\n",
    "\n",
    "    with open(\"results_summary.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4, default=convert_ndarray)\n",
    "\n",
    "    with open(\"results_summary.txt\", \"w\") as f:\n",
    "        for model, metrics in results.items():\n",
    "            f.write(f\"\\nModel: {model}\\n\")\n",
    "            f.write(f\"  Accuracy: {metrics.get('accuracy')}\\n\")\n",
    "            f.write(f\"  F1 Macro: {metrics.get('f1_macro')}\\n\")\n",
    "            f.write(f\"  Training Time (s): {metrics.get('training_time')}\\n\")\n",
    "            \n",
    "            f.write(\"  Optuna Params:\\n\")\n",
    "            for k, v in metrics.get(\"optuna_params\", {}).items():\n",
    "                f.write(f\"    - {k}: {v}\\n\")\n",
    "            \n",
    "            f.write(\"  Confusion Matrix:\\n\")\n",
    "            cm = np.array(metrics.get(\"cm\"))\n",
    "            for row in cm:\n",
    "                f.write(\"    \" + \" \".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "\n",
    "    for model in results:\n",
    "    # Save individual model params to JSON\n",
    "        with open(f\"{model.lower()}_optuna_best_params.json\", \"w\") as f:\n",
    "            json.dump(results[model].get('optuna_params', {}), f, indent=4, default=convert_ndarray)\n",
    "\n",
    "    # Save individual model params to TXT\n",
    "    with open(f\"{model.lower()}_optuna_best_params.txt\", \"w\") as f:\n",
    "        f.write(f\"{model} Best Optuna Parameters:\\n\")\n",
    "        for k, v in results[model].get('optuna_params', {}).items():\n",
    "            f.write(f\"- {k}: {v}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    save_results_to_csv(results)\n",
    "    plot_model_comparison(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the training and evaluation\n",
    "\n",
    "load_and_evaluate_all_models(augment=True, n_trials=2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ba8c6",
   "metadata": {},
   "source": [
    "# Visualization and Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375e0f9",
   "metadata": {},
   "source": [
    "Objective :-\n",
    "\n",
    "    The visualization function facilitates an intuitive comparative analysis of model performance by graphically representing macro-averaged F1 scores. This serves as a valuable tool for identifying the most effective model across evaluation tasks and is particularly well-suited for inclusion in academic presentations, reports, or publications.\n",
    "\n",
    "    the CSV export function enhances reproducibility and transparency by systematically storing key evaluation metrics—such as F1 scores, accuracy, training time, and hyperparameters in a structured, machine-readable format. This output is ideal for documentation in thesis appendices, tabulated result sections, or supplementary materials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e611886",
   "metadata": {},
   "source": [
    "## plot_model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results (dict): Dictionary containing performance metrics for each model\n",
    "def plot_model_comparison(results):\n",
    "    model_names = list(results.keys())\n",
    "    f1_scores = [results[m]['f1_macro'] for m in model_names]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(model_names, f1_scores)\n",
    "    plt.title(\"F1 Macro Score by Model\")\n",
    "    plt.ylabel(\"F1 Macro\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"f1_score_comparison.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d8383",
   "metadata": {},
   "source": [
    "## save_results_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports model evaluation metrics and hyperparameter configurations to a CSV file.\n",
    "def save_results_to_csv(results, file_path='final_results.csv'):\n",
    "    rows = []\n",
    "    for model_name, metrics in results.items():\n",
    "        rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"F1 Macro\": metrics.get(\"f1_macro\"),\n",
    "            \"Accuracy\": metrics.get(\"accuracy\"),\n",
    "            \"Training Time (s)\": metrics.get(\"training_time\"),\n",
    "            **metrics.get(\"optuna_params\", {})\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved CSV to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ff9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c64aaab0",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a17885",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d342935",
   "metadata": {},
   "source": [
    "/home2/sk23aib/ml.py:254:\n",
    "  trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "  \n",
    "    {'evalulation loss': 1.1640475988388062, 'evalulation accuracy': 0.8382905272579073, 'evalulation f1_macro': 0.6192348587152997, 'evalulation runtime': 37.898, 'evalulation_samples_per_second': 1650.986, 'evalulation_steps_per_second': 103.198, 'epoch': 3.0}\n",
    "    {'train_runtime': 2598.2224, 'train_samples_per_second': 337.14, 'train_steps_per_second': 21.072, 'train_loss': 1.2009653114249172, 'epoch': 3.0}\n",
    "Training time for BERT: 2598.51 seconds\n",
    "\n",
    "\n",
    "/home2/sk23aib/ml.py:228: \n",
    "  trainer = Trainer(\n",
    "\n",
    "BERT Results:\n",
    "\n",
    "  Accuracy: 0.8360849622017293\n",
    "  \n",
    "  F1 Macro: 0.622697837450507\n",
    "  \n",
    "  Classification Report:\n",
    "                   \n",
    "       \n",
    "                  precision   recall   f1-score  support\n",
    "       anger        0.77      0.58      0.66      2040\n",
    "     boredom        0.71      0.50      0.59        20\n",
    "       empty        0.50      0.55      0.52       925\n",
    "    enthusiasm      0.47      0.60      0.53      1541 \n",
    "         fun        0.60      0.53      0.56      1664\n",
    "    happiness       0.51      0.73      0.60      4469\n",
    "        hate        0.66      0.59      0.62      2112\n",
    "        love        0.83      0.66      0.73      6044\n",
    "     neutral        1.00      0.99      0.99     36216\n",
    "      relief        0.63      0.53      0.57      2744\n",
    "     sadness        0.55      0.69      0.61      2908\n",
    "    surprise        0.61      0.52      0.56      1176\n",
    "       worry        0.62      0.46      0.53       710\n",
    "\n",
    "    accuracy                           0.84       62569\n",
    "    macro avg        0.65      0.61      0.62     62569\n",
    "    weighted avg     0.85      0.84      0.84     62569\n",
    "\n",
    "\n",
    "Model and tokenizer saved to: ./bert_emotion_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8af26f",
   "metadata": {},
   "source": [
    "## Xlnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065358fb",
   "metadata": {},
   "source": [
    "\n",
    "Loading existing augmented dataset from: augmented_dataset.csv\n",
    "        \n",
    "        Label names: 'anger', 'boredom',  'relief', 'sadness', 'surprise', 'worry','empty', 'enthusiasm', 'fun', 'happiness', 'hate', 'love', 'neutral'\n",
    "Num of classes (num_labels): 13\n",
    "\n",
    "\n",
    "/home2/sk23aib/ml.py:254: \n",
    "  trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "  \n",
    "    {'evalulation_loss': 1.1928361654281616, 'evalulation_accuracy': 0.8404161805366875, 'evalulation_f1_macro': 0.6154764789644133, 'evalulation_runtime': 78.825, 'evalulation_samples_per_second': 793.771, evalulation'evalulation_steps_per_second': 49.616, 'epoch': 3.0}\n",
    "\n",
    "\n",
    "    {'train_runtime': 4709.4429, 'train_samples_per_second': 186.002, 'train_steps_per_second': 11.626, 'train_loss': 1.2257617823544158, 'epoch': 3.0}\n",
    "\n",
    "    Training time for XLNet: 4709.73 seconds\n",
    "\n",
    "\n",
    "/home2/sk23aib/ml.py:228: \n",
    "  trainer = Trainer(\n",
    "\n",
    "XLNet Results:\n",
    "  Accuracy: 0.8386261567229778\n",
    "  F1 Macro: 0.6049235095074379\n",
    "  Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "         \n",
    "                 precision    recall  f1-score   support\n",
    "       anger        0.61      0.63      0.62      2040\n",
    "     boredom        0.62      0.25      0.36        20\n",
    "       empty        0.63      0.52      0.57       925\n",
    "    enthusiasm      0.56      0.57      0.57      1541\n",
    "         fun        0.50      0.55      0.52      1664\n",
    "    happiness       0.55      0.70      0.62      4469\n",
    "        hate        0.68      0.58      0.62      2112\n",
    "        love        0.80      0.68      0.74      6044\n",
    "     neutral        0.99      1.00      1.00     36216\n",
    "      relief        0.59      0.54      0.57      2744\n",
    "     sadness        0.64      0.64      0.64      2908\n",
    "    surprise        0.53      0.55      0.54      1176\n",
    "       worry        0.52      0.52      0.52       710\n",
    "\n",
    "    accuracy                           0.84     62569\n",
    "    macro avg       0.63    0.59      0.60     62569\n",
    "    weighted avg    0.84    0.84      0.84     62569\n",
    "\n",
    "\n",
    "Model and tokenizer saved to: ./xlnet_emotion_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a395d",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4db826",
   "metadata": {},
   "source": [
    "\n",
    "Loading existing augmented dataset from: augmented_dataset.csv\n",
    "\n",
    "    Label names: 'anger', 'boredom',  'relief', 'sadness', 'surprise', 'worry','empty', 'enthusiasm', 'fun', 'happiness', 'hate', 'love', 'neutral'\n",
    "Num of classes (num_labels): 13\n",
    "\n",
    "/home2/sk23aib/ml.py:254: \n",
    "  trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "{'evalulation loss': 1.1552095413208008, 'evalulation accuracy': 0.8376831977496844, 'evalulation f1 macro': 0.6186894375669489, 'evalulation runtime': 41.7142, 'evalulation samples per_second': 1499.944, 'evalulation steps_per_second': 93.757, 'epoch': 3.0}\n",
    "\n",
    "\n",
    "{'train runtime': 2791.202, 'train samples per second': 313.83, 'train steps per/sec': 19.615, 'train loss': 1.2230425001170537, epoch: 3.0}\n",
    "Training time for GPT-2: 2791.45 seconds\n",
    "\n",
    "\n",
    "\n",
    "GPT-2 Results:\n",
    "  Accuracy: 0.8367242564209113\n",
    "  F1 Macro: 0.6146183418210642\n",
    "  Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "       anger       0.63      0.62      0.63      2040\n",
    "     boredom       0.89      0.40      0.55        20\n",
    "       empty       0.56      0.53      0.54       925\n",
    "    enthusiasm     0.51      0.58      0.55      1541\n",
    "         fun       0.55      0.53      0.54      1664\n",
    "    happiness      0.56      0.69      0.62      4469\n",
    "        hate       0.68      0.58      0.62      2112\n",
    "        love       0.80      0.69      0.74      6044\n",
    "     neutral       0.99      0.99      0.99     36216\n",
    "      relief       0.58      0.55      0.56      2744\n",
    "     sadness       0.64      0.65      0.64      2908\n",
    "    surprise       0.50      0.55      0.53      1176\n",
    "       worry       0.46      0.51      0.48       710\n",
    "\n",
    "    accuracy                           0.84     62569\n",
    "    macro avg       0.64      0.60      0.61     62569\n",
    "    weighted avg     0.84      0.84      0.84     62569\n",
    "\n",
    "\n",
    "Model and tokenizer saved to: ./gpt2_emotion_model\n",
    "\n",
    "Best model: BERT with F1 Macro: 0.622697837450507\n",
    "Saved CSV to final_results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc19686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "301.6px",
    "width": "470.6px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
